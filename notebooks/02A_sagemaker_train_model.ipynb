{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c809ea23-2572-41d7-8f2e-5e1dc2c10598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "bucket = \"verikai-heart-risk-pipeline\"\n",
    "prefix = \"data/processed_data\"\n",
    "region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79c1f12-a9e8-46ad-ba0c-26f7b2c8928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from datetime import datetime, timezone\n",
    "import sagemaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b252863a-97a1-4b34-8f5d-b66ba910f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŸ³ Loading local CSV â†’ /Users/rev/IUB/Projects/HeartAttackRiskPrediction/data/derived/phase4_pruned_all_latest_20250510_0445.csv\n",
      "Wrote local splits â†’ /Users/rev/IUB/Projects/HeartAttackRiskPrediction/data/derived/train.csv and validation.csv\n"
     ]
    }
   ],
   "source": [
    "project_root = Path.cwd().parent\n",
    "data_dir     = project_root / \"data\" / \"derived\"\n",
    "#Find the latest phase4 CSV \n",
    "candidates = list(data_dir.glob(\"phase4_pruned_all_latest_*.csv\"))\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(f\"No files matching phase4_pruned_all_latest_*.csv under {data_dir}\")\n",
    "latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
    "print(f\"âŸ³ Loading local CSV â†’ {latest_file}\")\n",
    "\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"heart_attack\"], random_state=42\n",
    ")\n",
    "df = df[[\"heart_attack\"] + [c for c in df.columns if c != \"heart_attack\"]]\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"heart_attack\"], random_state=42)\n",
    "\n",
    "out_dir = data_dir\n",
    "train_df.to_csv(out_dir / \"train.csv\",      index=False, header=False)\n",
    "val_df.to_csv(out_dir / \"validation.csv\",   index=False, header=False)\n",
    "print(f\"Wrote local splits â†’ {out_dir}/train.csv and validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d02319-c1d2-402f-b1c4-680629297274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Uploading to s3://verikai-heart-risk-pipeline/data/processed_data/train.csv\n",
      "â†’ Uploading to s3://verikai-heart-risk-pipeline/data/processed_data/validation.csv\n",
      "âœ… All splits uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# # parameters \n",
    "# bucket = \"verikai-heart-risk-pipeline\"\n",
    "# prefix = \"data/processed_data\"\n",
    "# region = \"us-east-1\"\n",
    "\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure an S3 client against the regional endpoint, forcing path-style as I got SSL error\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=region,\n",
    "    endpoint_url=f\"https://s3.{region}.amazonaws.com\",\n",
    "    config=Config(\n",
    "        signature_version=\"s3v4\",\n",
    "        s3={\"addressing_style\": \"path\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Multipartâ€upload config for large files\n",
    "transfer_cfg = TransferConfig(\n",
    "    multipart_threshold=25 * 1024 * 1024,   \n",
    "    multipart_chunksize=10 * 1024 * 1024,  \n",
    "    max_concurrency=4\n",
    ")\n",
    "\n",
    "def upload_split(df, key_name):\n",
    "    \"\"\"Write a DataFrame to S3 as CSV via multipart upload.\"\"\"\n",
    "    buf = StringIO()\n",
    "    df.to_csv(buf, index=False, header=False)\n",
    "    print(f\"â†’ Uploading to s3://{bucket}/{key_name}\")\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key_name,\n",
    "        Body=buf.getvalue()\n",
    "    )\n",
    "\n",
    "train_key = f\"{prefix}/train.csv\"\n",
    "val_key   = f\"{prefix}/validation.csv\"\n",
    "\n",
    "upload_split(train_df, train_key)\n",
    "upload_split(val_df,   val_key)\n",
    "\n",
    "print(\"All splits uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50d18b25-a3f9-4e71-9c4d-57e406678251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"heart_attack\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459f6663-09d5-460c-a9ec-2c3441aa43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "bucket = \"verikai-heart-risk-pipeline\"\n",
    "role = \"arn:aws:iam::904233112003:role/SageMakerExecutionRole-rev\"\n",
    "region = \"us-east-1\"\n",
    "prefix = \"data/processed_data\"\n",
    "\n",
    "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "output_path = f\"s3://{bucket}/models/heart_attack/{timestamp}/output\"\n",
    "job_name = f\"xgb-heart-{timestamp}\"\n",
    "\n",
    "#  SageMaker session \n",
    "session = sagemaker.Session(boto3.Session(region_name=region))\n",
    "container_uri = sagemaker.image_uris.retrieve(\"xgboost\", region, version=\"1.5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541fd5fb-e153-4b73-ae65-478fe5b20cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Estimator setup \n",
    "xgb = Estimator(\n",
    "    image_uri=container_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    base_job_name=\"xgb-heart\"\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=100,\n",
    "    max_depth=6,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b29851f-258a-44df-bf16-f66d405d0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{prefix}/train.csv\",\n",
    "    content_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{prefix}/validation.csv\",\n",
    "    content_type=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96a23e2-12ea-4178-bac9-7675ea93d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgb-heart-20250511T182913Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 18:29:25 Starting - Starting the training job...\n",
      "...5-05-11 18:29:41 Starting - Preparing the instances for training\n",
      "...5-05-11 18:30:04 Downloading - Downloading input data\n",
      "......5-11 18:30:59 Downloading - Downloading the training image\n",
      ".\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:55.447 ip-10-0-161-35.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:55.469 ip-10-0-161-35.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] Train matrix has 344604 rows and 44 columns\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] Validation matrix has 86151 rows\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:56.764 ip-10-0-161-35.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:56.765 ip-10-0-161-35.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:56.765 ip-10-0-161-35.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:56.765 ip-10-0-161-35.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-05-11:18:31:56:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:57.862 ip-10-0-161-35.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-05-11 18:31:57.866 ip-10-0-161-35.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.98787#011validation-auc:0.98735\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.98836#011validation-auc:0.98786\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.98860#011validation-auc:0.98795\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.98872#011validation-auc:0.98812\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.98881#011validation-auc:0.98823\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.98891#011validation-auc:0.98827\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.98887#011validation-auc:0.98827\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.98894#011validation-auc:0.98830\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.98905#011validation-auc:0.98834\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.98913#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.98922#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.98928#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.98935#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.98944#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.98950#011validation-auc:0.98835\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.98955#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.98960#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.98964#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.98969#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.98974#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.98979#011validation-auc:0.98833\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.98984#011validation-auc:0.98833\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.98990#011validation-auc:0.98834\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.98993#011validation-auc:0.98835\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.98996#011validation-auc:0.98836\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99001#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99004#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99009#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99011#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99015#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99018#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99020#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99026#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.99030#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.99033#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99035#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99043#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.99046#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.99049#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.99058#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99062#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.99065#011validation-auc:0.98836\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.99068#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99072#011validation-auc:0.98837\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.99084#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99086#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.99087#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.99089#011validation-auc:0.98838\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.99094#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99110#011validation-auc:0.98844\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.99111#011validation-auc:0.98844\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.99114#011validation-auc:0.98841\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.99115#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.99118#011validation-auc:0.98841\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.99122#011validation-auc:0.98841\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.99126#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.99127#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.99130#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.99132#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.99132#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.99134#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.99138#011validation-auc:0.98840\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.99140#011validation-auc:0.98839\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.99157#011validation-auc:0.98841\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.99182#011validation-auc:0.98848\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.99189#011validation-auc:0.98848\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.99198#011validation-auc:0.98850\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.99219#011validation-auc:0.98853\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.99220#011validation-auc:0.98852\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.99221#011validation-auc:0.98853\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.99228#011validation-auc:0.98854\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.99242#011validation-auc:0.98855\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.99245#011validation-auc:0.98855\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.99260#011validation-auc:0.98856\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.99270#011validation-auc:0.98856\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.99275#011validation-auc:0.98855\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.99281#011validation-auc:0.98857\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.99287#011validation-auc:0.98857\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.99295#011validation-auc:0.98860\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.99318#011validation-auc:0.98865\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.99323#011validation-auc:0.98863\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.99329#011validation-auc:0.98863\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.99338#011validation-auc:0.98863\u001b[0m\n",
      "\n",
      "\u001b[34m[83]#011train-auc:0.99343#011validation-auc:0.98863\u001b[0m model\n",
      "\u001b[34m[84]#011train-auc:0.99345#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.99351#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.99358#011validation-auc:0.98867\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.99363#011validation-auc:0.98866\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.99374#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.99377#011validation-auc:0.98865\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.99384#011validation-auc:0.98862\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.99389#011validation-auc:0.98863\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.99396#011validation-auc:0.98862\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.99401#011validation-auc:0.98862\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.99408#011validation-auc:0.98860\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.99415#011validation-auc:0.98859\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.99418#011validation-auc:0.98861\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.99423#011validation-auc:0.98859\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.99424#011validation-auc:0.98859\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.99426#011validation-auc:0.98857\u001b[0m\n",
      "\n",
      "2025-05-11 18:33:24 Completed - Training job completed\n",
      "Training seconds: 200\n",
      "Billable seconds: 200\n",
      "SageMaker training job submitted: xgb-heart-20250511T182913Z\n",
      "Output model will be in: s3://verikai-heart-risk-pipeline/models/heart_attack/20250511T182913Z/output/xgb-heart-20250511T182913Z/output/\n"
     ]
    }
   ],
   "source": [
    "# X_train = validate_features(X_train, \"data/derived/feature_list_final_curated.json\") -> Schema validation\n",
    "\n",
    "xgb.fit({\"train\": train_input, \"validation\": val_input}, job_name=job_name)\n",
    "\n",
    "print(f\"SageMaker training job submitted: {job_name}\")\n",
    "print(f\"Output model will be in: {output_path}/{job_name}/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "edb9232b-e521-487c-9402-d5364edd35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: sagemaker-xgboost-25-250510-1422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................!\n",
      "!\n",
      "Best training job: sagemaker-xgboost-25-250510-1422-008-0a47af4b\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter\n",
    "now = datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "tuning_job_name = f\"sagemaker-xgboost-{now}\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/latest_sagemaker_tuning_job.txt\", \"w\") as f:\n",
    "    f.write(tuning_job_name)\n",
    "xgb_hpo = HyperparameterTuner(\n",
    "    estimator          = xgb,\n",
    "    objective_metric_name=\"validation:auc\",\n",
    "    hyperparameter_ranges={\n",
    "        \"eta\": ContinuousParameter(0.1, 0.3),\n",
    "        \"max_depth\": IntegerParameter(4, 10),\n",
    "    },\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name=tuning_job_name\n",
    ")\n",
    "xgb_hpo.fit({\"train\": train_input, \"validation\": val_input})\n",
    "xgb_hpo.wait()\n",
    "\n",
    "best_job_name = xgb_hpo.best_training_job()\n",
    "print(\"Best training job:\", best_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "138bef3f-3ea3-4c3d-a4d2-169efea65004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-05-10 18:33:35 Starting - Found matching resource for reuse\n",
      "2025-05-10 18:33:35 Downloading - Downloading the training image\n",
      "2025-05-10 18:33:35 Training - Training image download completed. Training in progress.\n",
      "2025-05-10 18:33:35 Uploading - Uploading generated training model\n",
      "2025-05-10 18:33:35 Completed - Resource reused by training job: sagemaker-xgboost-25-250510-1422-010-07c2e525\n",
      "Best model S3 path: s3://verikai-heart-risk-pipeline/models/heart_attack/20250510T063154Z/output/sagemaker-xgboost-25-250510-1422-008-0a47af4b/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "best_estimator = sagemaker.estimator.Estimator.attach(best_job_name)\n",
    "print(\"Best model S3 path:\", best_estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8aa3fbe-279b-43ba-b345-5b7cd7f48343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-xgboost-250510-1352\n",
      "sagemaker-xgboost-250510-1327\n",
      "sagemaker-xgboost-250510-1305\n",
      "sagemaker-xgboost-25-250510-1422\n"
     ]
    }
   ],
   "source": [
    "# sm = boto3.client(\"sagemaker\")\n",
    "# tuning_jobs = sm.list_hyper_parameter_tuning_jobs(MaxResults=10)\n",
    "# for job in tuning_jobs[\"HyperParameterTuningJobSummaries\"]:\n",
    "#     print(job[\"HyperParameterTuningJobName\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9abdf-acf7-4e79-b963-ad6d6ecb8ebb",
   "metadata": {},
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Evaluate & MLflow-log best SageMaker model (with plots & categories)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e0f6b0d1-73fa-482f-b7d3-771e9678f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/10 17:12:30 INFO mlflow.tracking.fluent: Experiment with name 'Sagemaker-driven-heart-risk-model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-05-10 18:33:35 Starting - Found matching resource for reuse\n",
      "2025-05-10 18:33:35 Downloading - Downloading the training image\n",
      "2025-05-10 18:33:35 Training - Training image download completed. Training in progress.\n",
      "2025-05-10 18:33:35 Uploading - Uploading generated training model\n",
      "2025-05-10 18:33:35 Completed - Resource reused by training job: sagemaker-xgboost-25-250510-1422-010-07c2e525\n",
      "ðŸ› ï¸  Best training job: sagemaker-xgboost-25-250510-1422-008-0a47af4b\n",
      "âœ… Booster loaded\n",
      "âœï¸  Using header from: data/processed_data/phase4_pruned_all_latest_0445.csv\n",
      "âœ… Loaded validation: 86,151 rows Ã— 44 features\n",
      "âœ… Logged metrics: {'accuracy': 0.9705400981996727, 'precision': 0.6721049264235445, 'recall': 0.8959488272921109, 'f1_score': 0.7680497166879913, 'roc_auc': 0.9879486119288079}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/mlflow/xgboost/__init__.py:168: UserWarning: [17:12:40] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  xgb_model.save_model(model_data_path)\n",
      "\u001b[31m2025/05/10 17:12:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logged model to MLflow\n",
      "ðŸƒ View run sagemaker-xgboost-25-250510-1422-008-0a47af4b at: http://localhost:5001/#/experiments/3/runs/267c913ded474285b8b17456f34c9b99\n",
      "ðŸ§ª View experiment at: http://localhost:5001/#/experiments/3\n",
      "\n",
      "Validation metrics\n",
      "------------------\n",
      "Accuracy : 0.9705\n",
      "Precision: 0.6721\n",
      "Recall   : 0.8959\n",
      "F1-score : 0.7680\n",
      "ROC AUC  : 0.9879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import os, tempfile, tarfile, boto3, warnings\n",
    "# import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from io import StringIO\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"Sagemaker-driven-heart-risk-model\")\n",
    "\n",
    "# Attach best estimator and download model artifact\n",
    "best_job_name  = xgb_hpo.best_training_job()\n",
    "best_estimator = Estimator.attach(best_job_name)\n",
    "print(\"ðŸ› ï¸  Best training job:\", best_job_name)\n",
    "\n",
    "bucket      = \"verikai-heart-risk-pipeline\"\n",
    "model_key   = best_estimator.model_data.split(f\"s3://{bucket}/\")[-1]\n",
    "\n",
    "tmpdir      = tempfile.mkdtemp()\n",
    "local_tar   = os.path.join(tmpdir, \"model.tar.gz\")\n",
    "boto3.client(\"s3\").download_file(bucket, model_key, local_tar)\n",
    "\n",
    "# Suppress deprecation warning on tar\n",
    "with tarfile.open(local_tar) as tar:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "        tar.extractall(path=tmpdir)\n",
    "\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(os.path.join(tmpdir, \"xgboost-model\"))\n",
    "print(\"Booster loaded\")\n",
    "\n",
    "# Load and realign validation data\n",
    "s3         = boto3.client(\"s3\")\n",
    "raw_prefix = \"data/processed_data\"\n",
    "val_key    = f\"{raw_prefix}/validation.csv\"\n",
    "\n",
    "# fetch canonical feature order from full original CSV\n",
    "resp      = s3.list_objects_v2(Bucket=bucket, Prefix=raw_prefix + \"/\")\n",
    "all_csv   = [o[\"Key\"] for o in resp.get(\"Contents\",[]) if o[\"Key\"].endswith(\".csv\")]\n",
    "orig_csv  = sorted([k for k in all_csv if \"train.csv\" not in k and \"validation.csv\" not in k], reverse=True)[0]\n",
    "print(\"Using header from:\", orig_csv)\n",
    "\n",
    "header_df = pd.read_csv(StringIO(s3.get_object(Bucket=bucket, Key=orig_csv)[\"Body\"].read().decode(\"utf-8\")), nrows=0)\n",
    "cols      = header_df.columns.tolist()\n",
    "cols.remove(\"heart_attack\")\n",
    "\n",
    "# load val split (no header) and assign column names\n",
    "val_obj = s3.get_object(Bucket=bucket, Key=val_key)\n",
    "df_val  = pd.read_csv(StringIO(val_obj[\"Body\"].read().decode(\"utf-8\")), header=None)\n",
    "\n",
    "if df_val.shape[1] != len(cols) + 1:\n",
    "    raise ValueError(f\"Validation has {df_val.shape[1]} cols; expected {len(cols)+1}\")\n",
    "df_val.columns = [\"heart_attack\", *cols]\n",
    "\n",
    "X_val = df_val[cols].copy()\n",
    "y_val = df_val[\"heart_attack\"].astype(int).values\n",
    "print(f\"Loaded validation: {X_val.shape[0]:,} rows Ã— {X_val.shape[1]:,} features\")\n",
    "\n",
    "#Handle categorical features\n",
    "for col in X_val.select_dtypes(include=\"object\"):\n",
    "    X_val[col] = X_val[col].astype(\"category\")\n",
    "\n",
    "dval = xgb.DMatrix(X_val, enable_categorical=True)\n",
    "\n",
    "#Predict\n",
    "y_proba = bst.predict(dval)\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# Start MLflow run\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=best_job_name):\n",
    "    # 6.1 Log hyperparameters\n",
    "    for k, v in best_estimator.hyperparameters().items():\n",
    "        mlflow.log_param(k, v)\n",
    "\n",
    "    # 6.2 Log metrics\n",
    "    metrics = {\n",
    "        \"accuracy\":  accuracy_score(y_val, y_pred),\n",
    "        \"precision\": precision_score(y_val, y_pred),\n",
    "        \"recall\":    recall_score(y_val, y_pred),\n",
    "        \"f1_score\":  f1_score(y_val, y_pred),\n",
    "        \"roc_auc\":   roc_auc_score(y_val, y_proba)\n",
    "    }\n",
    "    for name, val in metrics.items():\n",
    "        mlflow.log_metric(name, val)\n",
    "    print(\"Logged metrics:\", metrics)\n",
    "\n",
    "    # Log model artifact\n",
    "    mlflow.xgboost.log_model(bst, artifact_path=\"xgb-model\")\n",
    "    print(\"Logged model to MLflow\")\n",
    "\n",
    "    RocCurveDisplay.from_predictions(y_val, y_proba)\n",
    "    plt.title(\"ROC Curve â€” Validation\")\n",
    "    roc_path = \"plots/roc_curve.png\"\n",
    "    plt.savefig(roc_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(roc_path)\n",
    "\n",
    "    # Save + log confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "                yticklabels=[\"True 0\", \"True 1\"])\n",
    "    plt.title(\"Confusion Matrix â€” Validation\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "    cm_path = \"plots/confusion_matrix_annotated.png\"\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(cm_path)\n",
    "\n",
    "print(f\"\"\"\n",
    "Validation metrics\n",
    "------------------\n",
    "Accuracy : {metrics['accuracy']:.4f}\n",
    "Precision: {metrics['precision']:.4f}\n",
    "Recall   : {metrics['recall']:.4f}\n",
    "F1-score : {metrics['f1_score']:.4f}\n",
    "ROC AUC  : {metrics['roc_auc']:.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f132a-b7bb-431e-8453-f2314704ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"model_artifact_s3\": f\"s3://{bucket}/{model_key}\",\n",
    "    \"training_job_name\": best_job_name\n",
    "}\n",
    "\n",
    "with open(\"/opt/airflow/out/notebook_output_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81039d-86aa-4211-91da-e7fbcde081f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
